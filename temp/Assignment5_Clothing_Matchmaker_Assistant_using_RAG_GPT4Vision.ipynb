{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yugesh18c/GenAIAssignments/blob/main/temp/Assignment5_Clothing_Matchmaker_Assistant_using_RAG_GPT4Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKKEcFOnDE8s"
      },
      "source": [
        "# Assignment 5: Clothing Match Maker Assistant using RAG with GPT4 Vision\n",
        "\n",
        "Welcome to the Clothing Matchmaker App Jupyter Notebook! This project demonstrates the power of the GPT-4V model in analyzing images of clothing items and extracting key features such as color, style, and type. The core of our app relies on this advanced image analysis model developed by OpenAI, which enables us to accurately identify the characteristics of the input clothing item.\n",
        "\n",
        "GPT-4V is a model that combines natural language processing with image recognition, allowing it to understand and generate responses based on both text and visual inputs.\n",
        "\n",
        "Building on the capabilities of the GPT-4V model, we employ a custom matching algorithm and the RAG technique to search our knowledge base for items that complement the identified features. This algorithm takes into account factors like color compatibility and style coherence to provide users with suitable recommendations. Through this notebook, we aim to showcase the practical application of these technologies in creating a clothing recommendation system.\n",
        "\n",
        "Using the combination of GPT-4 Vision + RAG (Retrieval-Augmented Generation) offers several advantages:\n",
        "\n",
        "1. **Contextual Understanding**: GPT-4 Vision can analyze input images and understand the context, such as the objects, scenes, and activities depicted. This allows for more accurate and relevant suggestions or information across various domains, whether it's interior design, cooking, or education.\n",
        "2. **Rich Knowledge Base**: RAG combines the generative capabilities of GPT-4 with a retrieval component that accesses a large corpus of information across different fields. This means the system can provide suggestions or insights based on a wide range of knowledge, from historical facts to scientific concepts.\n",
        "3. **Customization**: The approach allows for easy customization to cater to specific user needs or preferences in various applications. Whether it's tailoring suggestions to a user's taste in art or providing educational content based on a student's learning level, the system can be adapted to deliver personalized experiences.\n",
        "\n",
        "Overall, the GPT-4 Vision + RAG approach offers a powerful and flexible solution for various fashion-related applications, leveraging the strengths of both generative and retrieval-based AI techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjXABx7vDE8w"
      },
      "source": [
        "### Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP3ey18pDE8x"
      },
      "source": [
        "First we will install the necessary dependencies, then import the libraries and write some utility functions that we will use later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "GGZzY5g4DE8x"
      },
      "outputs": [],
      "source": [
        "!pip install openai tenacity tqdm numpy typing tiktoken  langchain langchain-core --quiet langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "TSi3YtwnfyJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "658644aa-f8d2-4695-d5fc-2925f60e6ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 30.0M  100 30.0M    0     0  22.2M      0  0:00:01  0:00:01 --:--:-- 22.2M\n"
          ]
        }
      ],
      "source": [
        "# use approprite command if curl is not installed to download docs, option: wget\n",
        "!curl -O https://raw.githubusercontent.com/anshupandey/Generative-AI-for-Professionals/main/datasets/sample_clothes.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "8vMMYdJtf3mb"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"sample_clothes.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"sample_clothes\")  # Creates a folder and extracts files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxPpiFDmi45F"
      },
      "source": [
        "## Initializes a connection to Azure OpenAI service for chat completions.\n",
        "### Configuration\n",
        "\n",
        "- Client Type: AzureChatOpenAI\n",
        "- Endpoint: Azure-hosted OpenAI service at clothassistant8618180812.openai.azure.com\n",
        "- Authentication: API key retrieved securely from Azure Key Vault\n",
        "- API Version: 2024-12-01-preview\n",
        "- Model: Uses gpt-4 model for chat completions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "fvKhh41KDE8z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import ast\n",
        "import tiktoken\n",
        "import concurrent\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from tqdm import tqdm\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from IPython.display import Image, display, HTML\n",
        "from typing import List\n",
        "import os\n",
        "from openai import AzureOpenAI\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "GPT_MODEL = \"gpt-4\"\n",
        "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
        "EMBEDDING_COST_PER_1K_TOKENS = 0.00013\n",
        "curr_path = os.getcwd()\n",
        "# Make sure to initialize env variables: for azure openai endpoint, key, api version\n",
        "\n",
        "\n",
        "# write code to initialize the Azure OpenAI Client\n",
        "#Environment setup and variables\n",
        "\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-15-preview\"\n",
        "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"text-embedding-ada-002\"\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '04f9a983b5d747baac8c74a75c0d525a'\n",
        "os.environ['OPENAI_ENDPOINT'] = 'https://swedencentral.api.cognitive.microsoft.com/'\n",
        "\n",
        "AZURE_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
        "\n",
        "# Azure client\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "    azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\")\n",
        ")\n",
        "\n",
        "AZURE_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
        "\n",
        "# Azure client\n",
        "client2 = AzureChatOpenAI(\n",
        "    azure_deployment=\"telcogpt\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "    azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n",
        "    openai_api_type=\"azure\",\n",
        "    temperature= 0\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qp6VjaaDE80"
      },
      "source": [
        "### Creating the Embeddings\n",
        "We will now set up the knowledge base by choosing a database and generating embeddings for it. I am using the `sample_styles.csv` file for this in the data folder. This is a sample of a bigger dataset that contains `~44K` items. This step can also be replaced by using an out-of-the-box vector database. For example, you can follow one of [these cookbooks](https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases) to set up your vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlBTzk6XDE81",
        "outputId": "05447f65-c9c6-462a-abeb-8f4e3cc77e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id gender masterCategory subCategory articleType baseColour  season  \\\n",
            "0  27152    Men        Apparel     Topwear      Shirts       Blue  Summer   \n",
            "1  10469    Men        Apparel     Topwear     Tshirts     Yellow    Fall   \n",
            "2  17169    Men        Apparel     Topwear      Shirts     Maroon    Fall   \n",
            "3  56702    Men        Apparel     Topwear      Kurtas       Blue  Summer   \n",
            "4  47062  Women        Apparel  Bottomwear     Patiala      Multi    Fall   \n",
            "\n",
            "     year   usage                       productDisplayName  \n",
            "0  2012.0  Formal       Mark Taylor Men Striped Blue Shirt  \n",
            "1  2011.0  Casual   Flying Machine Men Yellow Polo Tshirts  \n",
            "2  2011.0  Casual  U.S. Polo Assn. Men Checks Maroon Shirt  \n",
            "3  2012.0  Ethnic                  Fabindia Men Blue Kurta  \n",
            "4  2012.0  Ethnic        Shree Women Multi Colored Patiala  \n",
            "Opened dataset successfully. Dataset has 1000 items of clothing.\n"
          ]
        }
      ],
      "source": [
        "styles_filepath = os.path.join(curr_path, \"sample_clothes\", \"sample_clothes\", \"sample_styles.csv\")\n",
        "styles_df = pd.read_csv(styles_filepath, on_bad_lines='skip')\n",
        "print(styles_df.head())\n",
        "print(\"Opened dataset successfully. Dataset has {} items of clothing.\".format(len(styles_df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj4hCDb9DE81"
      },
      "source": [
        "Now we will generate embeddings for the entire dataset. We can parallelize the execution of these embeddings to ensure that the script scales up for larger datasets. With this logic, the time to create embeddings for the full `44K` entry dataset decreases from ~4h to ~2-3min."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "za7sxB-Ni45H"
      },
      "outputs": [],
      "source": [
        "## Batch Embedding Logic\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(10))\n",
        "def get_embeddings(input: list):\n",
        "    try:\n",
        "        # write code to use openai client and create embeddings, use variable 'response' to store embeddings\n",
        "\n",
        "        response = client.embeddings.create(\n",
        "            model=AZURE_DEPLOYMENT_NAME,  # Or use another embedding model\n",
        "            input=input\n",
        "        )\n",
        "\n",
        "        return [data.embedding for data in response.data]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_embeddings: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# Splits an iterable into batches of size n.\n",
        "def batchify(iterable, n=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iterable[ndx : min(ndx + n, l)]\n",
        "\n",
        "\n",
        "# Function for batching and parallel processing the embeddings\n",
        "def embed_corpus(\n",
        "    corpus: List[str],\n",
        "    batch_size=64,\n",
        "    num_workers=8,\n",
        "    max_context_len=8191,\n",
        "):\n",
        "    # Encode the corpus, truncating to max_context_len\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    encoded_corpus = [\n",
        "        encoded_article[:max_context_len] for encoded_article in encoding.encode_batch(corpus)\n",
        "    ]\n",
        "\n",
        "    # Calculate corpus statistics: the number of inputs, the total number of tokens, and the estimated cost to embed\n",
        "    num_tokens = sum(len(article) for article in encoded_corpus)\n",
        "    cost_to_embed_tokens = num_tokens / 1000 * EMBEDDING_COST_PER_1K_TOKENS\n",
        "    print(\n",
        "        f\"num_articles={len(encoded_corpus)}, num_tokens={num_tokens}, est_embedding_cost={cost_to_embed_tokens:.2f} USD\"\n",
        "    )\n",
        "\n",
        "    # Embed the corpus\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "\n",
        "        futures = [\n",
        "            executor.submit(get_embeddings, text_batch)\n",
        "            for text_batch in batchify(encoded_corpus, batch_size)\n",
        "        ]\n",
        "\n",
        "        with tqdm(total=len(encoded_corpus)) as pbar:\n",
        "            for _ in concurrent.futures.as_completed(futures):\n",
        "                pbar.update(batch_size)\n",
        "\n",
        "        embeddings = []\n",
        "        for future in futures:\n",
        "            data = future.result()\n",
        "            embeddings.extend(data)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Function to generate embeddings for a given column in a DataFrame\n",
        "def generate_embeddings(df, column_name):\n",
        "    # Initialize an empty list to store embeddings\n",
        "    descriptions = df[column_name].astype(str).tolist()\n",
        "    embeddings = embed_corpus(descriptions)\n",
        "\n",
        "    # Add the embeddings as a new column to the DataFrame\n",
        "    df['embeddings'] = embeddings\n",
        "    print(\"Embeddings created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1YK1uOyDE82"
      },
      "source": [
        "#### Two options for creating the embeddings:\n",
        "The next line will **create the embeddings** for the sample clothes dataset. This will take around 0.02s to process and another ~30s to write the results to a local .csv file. The process is using our `text_embedding_3_large` model which is priced at `$0.00013/1K` tokens. Given that the dataset has around `1K` entries, the following operation will cost approximately `$0.001`. If you decide to work with the entire dataset of `44K` entries, this operation will take 2-3min to process and it will cost approximately `$0.07`.\n",
        "\n",
        "**If you would not like to proceed with creating your own embeddings**, we will use a dataset of pre-computed embeddings. You can skip this cell and uncomment the code in the following cell to proceed with loading the pre-computed vectors. This operation takes ~1min to load all the data in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX3UPTwKDE82",
        "outputId": "45fe1fd3-7c76-4636-fb6b-a65b80383158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_articles=1000, num_tokens=8280, est_embedding_cost=0.00 USD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1024it [00:02, 361.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings created successfully.\n",
            "Writing embeddings to file ...\n",
            "Embeddings successfully stored in sample_styles_with_embeddings.csv\n"
          ]
        }
      ],
      "source": [
        "generate_embeddings(styles_df, 'productDisplayName')\n",
        "print(\"Writing embeddings to file ...\")\n",
        "styles_df.to_csv('sample_clothes/sample_styles_with_embeddings.csv', index=False)\n",
        "print(\"Embeddings successfully stored in sample_styles_with_embeddings.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96x5kXBDE83",
        "outputId": "c217ad3f-09ac-4232-835c-3357c5225f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      id gender masterCategory subCategory articleType baseColour  season  \\\n",
            "0  27152    Men        Apparel     Topwear      Shirts       Blue  Summer   \n",
            "1  10469    Men        Apparel     Topwear     Tshirts     Yellow    Fall   \n",
            "2  17169    Men        Apparel     Topwear      Shirts     Maroon    Fall   \n",
            "3  56702    Men        Apparel     Topwear      Kurtas       Blue  Summer   \n",
            "4  47062  Women        Apparel  Bottomwear     Patiala      Multi    Fall   \n",
            "\n",
            "     year   usage                       productDisplayName  \\\n",
            "0  2012.0  Formal       Mark Taylor Men Striped Blue Shirt   \n",
            "1  2011.0  Casual   Flying Machine Men Yellow Polo Tshirts   \n",
            "2  2011.0  Casual  U.S. Polo Assn. Men Checks Maroon Shirt   \n",
            "3  2012.0  Ethnic                  Fabindia Men Blue Kurta   \n",
            "4  2012.0  Ethnic        Shree Women Multi Colored Patiala   \n",
            "\n",
            "                                          embeddings  \n",
            "0  [-0.03645945340394974, -0.01935923472046852, 0...  \n",
            "1  [-0.026362190023064613, -0.029589686542749405,...  \n",
            "2  [-0.030392521992325783, -0.006195900496095419,...  \n",
            "3  [0.0021314204204827547, -0.0003071317332796752...  \n",
            "4  [-0.022628404200077057, 0.0025569009594619274,...  \n",
            "Opened dataset successfully. Dataset has 1000 items of clothing along with their embeddings.\n"
          ]
        }
      ],
      "source": [
        "# styles_df = pd.read_csv('sample_clothes/sample_styles_with_embeddings.csv', on_bad_lines='skip')\n",
        "\n",
        "# # Convert the 'embeddings' column from string representations of lists to actual lists of floats\n",
        "# styles_df['embeddings'] = styles_df['embeddings'].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "print(styles_df.head())\n",
        "print(\"Opened dataset successfully. Dataset has {} items of clothing along with their embeddings.\".format(len(styles_df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFROgJllDE86"
      },
      "source": [
        "### Building the Matching Algorithm\n",
        "\n",
        "In this section, we'll develop a cosine similarity retrieval algorithm to find similar items in our dataframe. We'll utilize our custom cosine similarity function for this purpose. While the `sklearn` library offers a built-in cosine similarity function, recent updates to its SDK have led to compatibility issues, prompting us to implement our own standard cosine similarity calculation.\n",
        "\n",
        "If you already have a vector database set up, you can skip this step. Most standard databases come with their own search functions, which simplify the subsequent steps outlined in this guide. However, we aim to demonstrate that the matching algorithm can be tailored to meet specific requirements, such as a particular threshold or a specified number of matches returned.\n",
        "\n",
        "The `find_similar_items` function accepts four parameters:\n",
        "- `embedding`: The embedding for which we want to find a match.\n",
        "- `embeddings`: A list of embeddings to search through for the best matches.\n",
        "- `threshold` (optional): This parameter specifies the minimum similarity score for a match to be considered valid. A higher threshold results in closer (better) matches, while a lower threshold allows for more items to be returned, though they may not be as closely matched to the initial `embedding`.\n",
        "- `top_k` (optional): This parameter determines the number of items to return that exceed the given threshold. These will be the top-scoring matches for the provided `embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "2JS1nnHnDE86"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity_manual(vec1, vec2):\n",
        "    vec1 = np.array(vec1).reshape(1, -1)  # ensure 2D\n",
        "    vec2 = np.array(vec2).reshape(1, -1)  # ensure 2D\n",
        "    return cosine_similarity(vec1, vec2)[0][0]\n",
        "\n",
        "\n",
        "def find_similar_items(input_embedding, embeddings, threshold=0.5, top_k=2):\n",
        "    \"\"\"Find the most similar items based on cosine similarity.\"\"\"\n",
        "\n",
        "    # Calculate cosine similarity between the input embedding and all other embeddings\n",
        "    similarities = [(index, cosine_similarity_manual(input_embedding, vec)) for index, vec in enumerate(embeddings)]\n",
        "\n",
        "    # Filter out any similarities below the threshold\n",
        "    filtered_similarities = [(index, sim) for index, sim in similarities if sim >= threshold]\n",
        "\n",
        "    # Sort the filtered similarities by similarity score\n",
        "    sorted_indices = sorted(filtered_similarities, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "    # Return the top-k most similar items\n",
        "    return sorted_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "kkYU4s64DE87"
      },
      "outputs": [],
      "source": [
        "def find_matching_items_with_rag(df_items, item_descs):\n",
        "    \"\"\"Take the input item descriptions and find the most similar items based on cosine similarity for each description.\"\"\"\n",
        "\n",
        "    # Select the embeddings from the DataFrame\n",
        "    embeddings = df_items['embeddings'].tolist()\n",
        "\n",
        "    similar_items = []\n",
        "    for desc in item_descs:\n",
        "\n",
        "        # Generate the embedding for the input item\n",
        "        input_embedding = get_embeddings([desc])[0]  # Make sure this returns a single embedding vector\n",
        "\n",
        "        # Find the most similar items based on cosine similarity\n",
        "        similar_indices = find_similar_items(input_embedding, embeddings, threshold=0.6)\n",
        "\n",
        "        # Ensure all indices are integers\n",
        "        similar_indices = [int(i) for i in similar_indices if isinstance(i, int) or str(i).isdigit()]\n",
        "\n",
        "        # Get rows using valid indices\n",
        "        for i in similar_indices:\n",
        "            if 0 <= i < len(df_items):\n",
        "                similar_items.append(df_items.iloc[i])\n",
        "\n",
        "    return similar_items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCM-AkL_DE87"
      },
      "source": [
        "### Analysis Module\n",
        "\n",
        "In this module, we leverage `gpt-4-vision-preview` to analyze input images and extract important features like detailed descriptions, styles, and types. The analysis is performed through a straightforward API call, where we provide the URL of the image for analysis and request the model to identify relevant features.\n",
        "\n",
        "To ensure the model returns accurate results, we use specific techniques in our prompt:\n",
        "\n",
        "1. **Output Format Specification**: We instruct the model to return a JSON block with a predefined structure, consisting of:\n",
        "   - `items` (str[]): A list of strings, each representing a concise title for an item of clothing, including style, color, and gender. These titles closely resemble the `productDisplayName` property in our original database.\n",
        "   - `category` (str): The category that best represents the given item. The model selects from a list of all unique `articleTypes` present in the original styles dataframe.\n",
        "   - `gender` (str): A label indicating the gender the item is intended for. The model chooses from the options `[Men, Women, Boys, Girls, Unisex]`.\n",
        "\n",
        "2. **Clear and Concise Instructions**:\n",
        "   - We provide clear instructions on what the item titles should include and what the output format should be. The output should be in JSON format, but without the `json` tag that the model response normally contains.\n",
        "\n",
        "3. **One Shot Example**:\n",
        "   - To further clarify the expected output, we provide the model with an example input description and a corresponding example output. Although this may increase the number of tokens used (and thus the cost of the call), it helps to guide the model and results in better overall performance.\n",
        "\n",
        "By following this structured approach, we aim to obtain precise and useful information from the `gpt-4-vision-preview` model for further analysis and integration into our database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "ZONhHlycDE88"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def analyze_image(image_base64, subcategories):\n",
        "    # Prepare the prompt and image inputs for AzureChatOpenAI\n",
        "    messages = [\n",
        "        HumanMessage(\n",
        "            content=[\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": f\"\"\"Given an image of an item of clothing, analyze the item and generate a JSON output with the following fields: \"items\", \"category\", and \"gender\".\n",
        "                           Use your understanding of fashion trends, styles, and gender preferences to provide accurate and relevant suggestions for how to complete the outfit.\n",
        "                           The items field should be a list of items that would go well with the item in the picture. Each item should represent a title of an item of clothing that contains the style, color, and gender of the item.\n",
        "                           The category needs to be chosen between the types in this list: {subcategories}.\n",
        "                           You have to choose between the genders in this list: [Men, Women, Boys, Girls, Unisex]\n",
        "                           Do not include the description of the item in the picture. Do not include the ```json ``` tag in the output.\n",
        "\n",
        "                           Example Input: An image representing a black leather jacket.\n",
        "\n",
        "                           Example Output: {{\"items\": [\"Fitted White Women's T-shirt\", \"White Canvas Sneakers\", \"Women's Black Skinny Jeans\"], \"category\": \"Jackets\", \"gender\": \"Women\"}}\"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                      \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Call the AzureChatOpenAI client to fetch send the messages and get response and return the response\n",
        "\n",
        "    try:\n",
        "        response = client2.invoke(messages)\n",
        "        return response.content  # Should be the JSON string as expected\n",
        "    except Exception as e:\n",
        "        print(f\"Error in analyze_image: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEyM7iPFDE89"
      },
      "source": [
        "### Testing the Prompt with Sample Images\n",
        "\n",
        "To evaluate the effectiveness of our prompt, let's load and test it with a selection of images from our dataset. We'll use images from the `\"data/sample_clothes/sample_images\"` folder, ensuring a variety of styles, genders, and types. Here are the chosen samples:\n",
        "\n",
        "- `2133.jpg`: Men's shirt\n",
        "- `7143.jpg`: Women's shirt\n",
        "- `4226.jpg`: Casual men's printed t-shirt\n",
        "\n",
        "By testing the prompt with these diverse images, we can assess its ability to accurately analyze and extract relevant features from different types of clothing items and accessories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJU0PUDhDE89"
      },
      "source": [
        "We need a utility function to encode the .jpg images in base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "ryTjO5JoDE8-"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "def encode_image_to_base64(image_path):\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        encoded_image = base64.b64encode(image_file.read())\n",
        "        return encoded_image.decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "snkMbLBHDE8-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "2f623ea8-c59f-40f8-d989-db9c0d383bd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "# Set the path to the images and select a test image\n",
        "image_path = os.path.join(curr_path, \"sample_clothes\", \"sample_clothes\", \"sample_images\")\n",
        "test_images = [\"2133.jpg\", \"7143.jpg\", \"4226.jpg\"]\n",
        "\n",
        "# Encode the test image to base64\n",
        "reference_image = os.path.join(image_path, test_images[0])\n",
        "encoded_image = encode_image_to_base64(reference_image)\n",
        "encoded_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "M3saMVj9DE8-",
        "outputId": "878a47f7-b514-488b-8bb6-06a6d3db89bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'items': [\"Light Blue Men's Jeans\", 'Brown Casual Shoes', 'Grey Cotton Shorts'], 'category': 'Shirts', 'gender': 'Men'}\n"
          ]
        }
      ],
      "source": [
        "# Select the unique subcategories from the DataFrame\n",
        "unique_subcategories = styles_df['articleType'].unique()\n",
        "\n",
        "# Analyze the image and return the results\n",
        "analysis = analyze_image(encoded_image, unique_subcategories)\n",
        "image_analysis = json.loads(analysis)\n",
        "\n",
        "# Display the image and the analysis results\n",
        "display(Image(filename=reference_image))\n",
        "print(image_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Gg9HRdDE8-"
      },
      "source": [
        "Next, we process the output from the image analysis and use it to filter and display matching items from our dataset. Here's a breakdown of the code:\n",
        "\n",
        "1. **Extracting Image Analysis Results**: We extract the item descriptions, category, and gender from the `image_analysis` dictionary.\n",
        "\n",
        "2. **Filtering the Dataset**: We filter the `styles_df` DataFrame to include only items that match the gender from the image analysis (or are unisex) and exclude items of the same category as the analyzed image.\n",
        "\n",
        "3. **Finding Matching Items**: We use the `find_matching_items_with_rag` function to find items in the filtered dataset that match the descriptions extracted from the analyzed image.\n",
        "\n",
        "4. **Displaying Matching Items**: We create an HTML string to display images of the matching items. We construct the image paths using the item IDs and append each image to the HTML string. Finally, we use `display(HTML(html))` to render the images in the notebook.\n",
        "\n",
        "This cell effectively demonstrates how to use the results of image analysis to filter a dataset and visually display items that match the analyzed image's characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am0LcZ1EDE8_",
        "outputId": "2e41fdc2-b9d2-4464-cc51-76e812f17689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513 Remaining Items\n",
            "Length= 0\n",
            "[\"Light Blue Men's Jeans\", 'Brown Casual Shoes', 'Grey Cotton Shorts']\n",
            "Length= 0\n"
          ]
        }
      ],
      "source": [
        "# Extract the relevant features from the analysis\n",
        "item_descs = image_analysis['items']\n",
        "item_category = image_analysis['category']\n",
        "item_gender = image_analysis['gender']\n",
        "\n",
        "\n",
        "# Filter data such that we only look through the items of the same gender (or unisex) and different category\n",
        "filtered_items = styles_df.loc[styles_df['gender'].isin([item_gender, 'Unisex'])]\n",
        "filtered_items = filtered_items[filtered_items['articleType'] != item_category]\n",
        "print(str(len(filtered_items)) + \" Remaining Items\")\n",
        "\n",
        "# Find the most similar items based on the input item descriptions\n",
        "matching_items = find_matching_items_with_rag(filtered_items, item_descs)\n",
        "\n",
        "# Display the matching items (this will display 2 items for each description in the image analysis)\n",
        "html = \"\"\n",
        "paths = []\n",
        "print(\"Length=\",len(matching_items))\n",
        "for i, item in enumerate(matching_items):\n",
        "    item_id = item['id']\n",
        "\n",
        "    # Path to the image file\n",
        "    image_path = os.path.join(curr_path, \"sample_clothes\", \"sample_clothes\", \"sample_images\", f'{item_id}.jpg')\n",
        "    paths.append(image_path)\n",
        "    html += f'<img src=\"{image_path}\" style=\"display:inline;margin:1px\"/>'\n",
        "    print(\"Length=\",len(paths))\n",
        "# Print the matching item description as a reminder of what we are looking for\n",
        "print(item_descs)\n",
        "# Display the image\n",
        "for pth in paths:\n",
        "  display(Image(filename=pth))\n",
        "print(\"Length=\",len(paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl7fGGHLDE8_"
      },
      "source": [
        "### Guardrails\n",
        "\n",
        "In the context of using Large Language Models (LLMs) like GPT-4V, \"guardrails\" refer to mechanisms or checks put in place to ensure that the model's output remains within desired parameters or boundaries. These guardrails are crucial for maintaining the quality and relevance of the model's responses, especially when dealing with complex or nuanced tasks.\n",
        "\n",
        "Guardrails are useful for several reasons:\n",
        "\n",
        "1. **Accuracy**: They help ensure that the model's output is accurate and relevant to the input provided.\n",
        "2. **Consistency**: They maintain consistency in the model's responses, especially when dealing with similar or related inputs.\n",
        "3. **Safety**: They prevent the model from generating harmful, offensive, or inappropriate content.\n",
        "4. **Contextual Relevance**: They ensure that the model's output is contextually relevant to the specific task or domain it is being used for.\n",
        "\n",
        "In our case, we are using GPT-4V to analyze fashion images and suggest items that would complement an original outfit. To implement guardrails, we can **refine results**: After obtaining initial suggestions from GPT-4V, we can send the original image and the suggested items back to the model. We can then ask GPT-4V to evaluate whether each suggested item would indeed be a good fit for the original outfit.\n",
        "\n",
        "This gives the model the ability to self-correct and adjust its own output based on feedback or additional information. By implementing these guardrails and enabling self-correction, we can enhance the reliability and usefulness of the model's output in the context of fashion analysis and recommendation.\n",
        "\n",
        "To facilitate this, we write a prompt that asks the LLM for a simple \"yes\" or \"no\" answer to the question of whether the suggested items match the original outfit or not. This binary response helps streamline the refinement process and ensures clear and actionable feedback from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "hJKF2I6yDE9A"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def check_match2(reference_image_base64, suggested_image_base64):\n",
        "    # Prepare the prompt and image inputs for AzureChatOpenAI\n",
        "    \"\"\"\n",
        "    Determines if two clothing items, represented by their base64-encoded images, would work well together in an outfit using an LLM-based vision model.\n",
        "\n",
        "    Guidelines for implementing this function:\n",
        "    1. **Inputs**:\n",
        "        - `reference_image_base64` (str): Base64-encoded string of the reference clothing item image (the item to be matched).\n",
        "        - `suggested_image_base64` (str): Base64-encoded string of the suggested clothing item image (the candidate match).\n",
        "\n",
        "    2. **Prompt Construction**:\n",
        "        - Construct a prompt that clearly instructs the model to compare the two clothing items and decide if they would work well together in an outfit.\n",
        "        - The prompt should specify:\n",
        "            - The first image is the reference item.\n",
        "            - The second image is the suggested item.\n",
        "            - The model must output a JSON object with two fields: \"answer\" (either \"yes\" or \"no\") and \"reason\" (a brief explanation).\n",
        "            - The explanation should not include descriptions of the images themselves.\n",
        "            - The output must not include the ```json``` tag.\n",
        "\n",
        "    3. **Message Formatting**:\n",
        "        - Use the `HumanMessage` class to format the prompt and attach both images as base64-encoded image URLs.\n",
        "        - The message content should be a list containing:\n",
        "            - The prompt text (as described above).\n",
        "            - The reference image (as a dict with type \"image_url\" and the base64 string).\n",
        "            - The suggested image (as a dict with type \"image_url\" and the base64 string).\n",
        "\n",
        "    4. **Model Invocation**:\n",
        "        - Use the AzureChatOpenAI client (assumed to be initialized as `client`) to send the message.\n",
        "        - Set an appropriate `max_tokens` limit (e.g., 300) to ensure concise output.\n",
        "\n",
        "    5. **Output Handling**:\n",
        "        - Extract the model's response content.\n",
        "        - Return the response as a string (expected to be a JSON object with \"answer\" and \"reason\").\n",
        "\n",
        "    6. **Error Handling**:\n",
        "        - Optionally, add error handling to manage cases where the model's response is not valid JSON or does not contain the expected fields.\n",
        "\n",
        "    Example output:\n",
        "        {\n",
        "            \"answer\": \"yes\",\n",
        "            \"reason\": \"The suggested item complements the reference item in both color and style, making them suitable for an outfit together.\"\n",
        "        }\n",
        "    \"\"\"\n",
        "    # implement this function\n",
        "\n",
        "    raise NotImplementedError(\"This function is not yet implemented\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "import json\n",
        "\n",
        "def check_match(reference_image_base64, suggested_image_base64):\n",
        "    try:\n",
        "        # Prepare the prompt and image inputs\n",
        "        messages = [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": (\n",
        "                            \"You are a fashion expert. Determine whether the two clothing items shown in the images \"\n",
        "                            \"would work well together in an outfit.\\n\\n\"\n",
        "                            \"- The first image is the **reference item** (main piece).\\n\"\n",
        "                            \"- The second image is the **suggested item** (to be paired with the reference).\\n\\n\"\n",
        "                            \"Provide your answer as a JSON object with the following fields:\\n\"\n",
        "                            \"1. `answer`: Either \\\"yes\\\" or \\\"no\\\".\\n\"\n",
        "                            \"2. `reason`: A short explanation justifying your choice.\\n\\n\"\n",
        "                            \"Do not describe the items directly. Do not include the ```json``` tag.\"\n",
        "                        )\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{reference_image_base64}\"}\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{suggested_image_base64}\"}\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Send the message to the AzureChatOpenAI client\n",
        "        response = client.invoke(messages, max_tokens=300)\n",
        "\n",
        "        # Extract and return the JSON output\n",
        "        if hasattr(response, 'content'):\n",
        "            return response.content\n",
        "        else:\n",
        "            raise ValueError(\"Model response does not contain expected 'content' field.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        return json.dumps({\n",
        "            \"answer\": \"no\",\n",
        "            \"reason\": f\"Error in check_match: {str(e)}\"\n",
        "        })\n"
      ],
      "metadata": {
        "id": "KR1SmJvt6I87"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_EOmR1UDE9C"
      },
      "source": [
        "Finally, let's determine which of the items identified above truly complement the outfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZc8OAnPDE-L",
        "outputId": "75560ab7-d6ff-40ae-8526-3360f4171af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# Select the unique paths for the generated images\n",
        "paths = list(set(paths))\n",
        "\n",
        "for path in paths:\n",
        "\n",
        "    # Encode the test image to base64\n",
        "    suggested_image = encode_image_to_base64(path)\n",
        "    print(suggested_image)\n",
        "    # Check if the items match\n",
        "    match = json.loads(check_match(encoded_image, suggested_image))\n",
        "\n",
        "    # Display the image and the analysis results\n",
        "    if match[\"answer\"] == 'yes':\n",
        "        display(Image(filename=path))\n",
        "        print(\"The items match!\")\n",
        "        print(match[\"reason\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJjbfT95DE-L"
      },
      "source": [
        "We can observe that the initial list of potential items has been further refined, resulting in a more curated selection that aligns well with the outfit. Additionally, the model provides explanations for why each item is considered a good match, offering valuable insights into the decision-making process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgEnKp2wDE-M"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "In this Jupyter Notebook, we explored the application of GPT-4 with Vision and other machine learning techniques to the domain of fashion. We demonstrated how to analyze images of clothing items, extract relevant features, and use this information to find matching items that complement an original outfit. Through the implementation of guardrails and self-correction mechanisms, we refined the model's suggestions to ensure they are accurate and contextually relevant.\n",
        "\n",
        "This approach has several practical uses in the real world, including:\n",
        "\n",
        "1. **Personalized Shopping Assistants**: Retailers can use this technology to offer personalized outfit recommendations to customers, enhancing the shopping experience and increasing customer satisfaction.\n",
        "2. **Virtual Wardrobe Applications**: Users can upload images of their own clothing items to create a virtual wardrobe and receive suggestions for new items that match their existing pieces.\n",
        "3. **Fashion Design and Styling**: Fashion designers and stylists can use this tool to experiment with different combinations and styles, streamlining the creative process.\n",
        "\n",
        "However, one of the considerations to keep in mind is **cost**. The use of LLMs and image analysis models can incur costs, especially if used extensively. It's important to consider the cost-effectiveness of implementing these technologies. `gpt-4-vision-preview` is priced at `$0.01` per 1000 tokens. This adds up to `$0.00255` for one 256px x 256px image.\n",
        "\n",
        "Overall, this notebook serves as a foundation for further exploration and development in the intersection of fashion and AI, opening doors to more personalized and intelligent fashion recommendation systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "jK-vqaemlcSt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}