{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKDTT46etggzzo3UKcu/6B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yugesh18c/GenAIAssignments/blob/main/Assignment4/AS4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlB7fEIur6lg",
        "outputId": "f6d93339-2bbf-49a8-91bb-b92ae2a024e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n"
          ]
        }
      ],
      "source": [
        "print(\"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai faiss-cpu pandas tqdm --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss8nRnRRv8lb",
        "outputId": "88e39de2-71db-4bca-ffa3-8d4f5412447b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importaing libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from typing import List, Tuple\n",
        "from openai import AzureOpenAI"
      ],
      "metadata": {
        "id": "mJ7OKOhcwA3a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Environment setup and variables\n",
        "\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-15-preview\"\n",
        "os.environ[\"OPENAI_DEPLOYMENT_NAME\"] = \"text-embedding-ada-002\"\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '04f9a983b5d747baac8c74a75c0d525a'\n",
        "os.environ['OPENAI_ENDPOINT'] = 'https://swedencentral.api.cognitive.microsoft.com/'"
      ],
      "metadata": {
        "id": "PSlzI6iOv5-J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AZURE_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
        "\n",
        "# Azure client\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "    azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\")\n",
        ")"
      ],
      "metadata": {
        "id": "Ox8NYjmrwd58"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json('/content/sample_data/self_critique_loop_dataset.json') #Read locally uploaded JSON file. This path will change based on where JSON file is placed\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RLDbD3kVn-JO",
        "outputId": "8b144e49-c8b1-4ecc-b51a-8a5ae1aeb6b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  doc_id                                           question  \\\n",
              "0  KB001             What are best practices for debugging?   \n",
              "1  KB002    What are best practices for performance tuning?   \n",
              "2  KB003               What are best practices for caching?   \n",
              "3  KB004  What are best practices for asynchronous progr...   \n",
              "4  KB005        What are best practices for API versioning?   \n",
              "\n",
              "                                      answer_snippet  \\\n",
              "0  When addressing debugging, it's important to f...   \n",
              "1  When addressing performance tuning, it's impor...   \n",
              "2  When addressing caching, it's important to fol...   \n",
              "3  When addressing asynchronous programming, it's...   \n",
              "4  When addressing API versioning, it's important...   \n",
              "\n",
              "                              source confidence_indicator last_updated  \n",
              "0                 debugging_guide.md             moderate   2024-01-10  \n",
              "1        performance tuning_guide.md             moderate   2024-02-10  \n",
              "2                   caching_guide.md             moderate   2024-03-10  \n",
              "3  asynchronous programming_guide.md             moderate   2024-04-10  \n",
              "4            API versioning_guide.md             moderate   2024-05-10  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-689fbb88-3c81-4b38-9ce2-db54e8769890\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_snippet</th>\n",
              "      <th>source</th>\n",
              "      <th>confidence_indicator</th>\n",
              "      <th>last_updated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KB001</td>\n",
              "      <td>What are best practices for debugging?</td>\n",
              "      <td>When addressing debugging, it's important to f...</td>\n",
              "      <td>debugging_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-01-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KB002</td>\n",
              "      <td>What are best practices for performance tuning?</td>\n",
              "      <td>When addressing performance tuning, it's impor...</td>\n",
              "      <td>performance tuning_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-02-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KB003</td>\n",
              "      <td>What are best practices for caching?</td>\n",
              "      <td>When addressing caching, it's important to fol...</td>\n",
              "      <td>caching_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-03-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KB004</td>\n",
              "      <td>What are best practices for asynchronous progr...</td>\n",
              "      <td>When addressing asynchronous programming, it's...</td>\n",
              "      <td>asynchronous programming_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-04-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KB005</td>\n",
              "      <td>What are best practices for API versioning?</td>\n",
              "      <td>When addressing API versioning, it's important...</td>\n",
              "      <td>API versioning_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-05-10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-689fbb88-3c81-4b38-9ce2-db54e8769890')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-689fbb88-3c81-4b38-9ce2-db54e8769890 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-689fbb88-3c81-4b38-9ce2-db54e8769890');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a64fd25f-ead4-4ae1-b020-8fafe26bb4f6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a64fd25f-ead4-4ae1-b020-8fafe26bb4f6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a64fd25f-ead4-4ae1-b020-8fafe26bb4f6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"KB028\",\n          \"KB016\",\n          \"KB024\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"What are best practices for error handling?\",\n          \"What are best practices for performance tuning?\",\n          \"What are best practices for unit testing?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_snippet\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"When addressing error handling, it's important to follow well-defined patterns...\",\n          \"When addressing performance tuning, it's important to follow well-defined patterns...\",\n          \"When addressing unit testing, it's important to follow well-defined patterns...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"error handling_guide.md\",\n          \"performance tuning_guide.md\",\n          \"unit testing_guide.md\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confidence_indicator\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"moderate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_updated\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"2024-01-10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "VyyYVDI3ocFO",
        "outputId": "2a065a02-072f-4fc7-d30b-7b5c58578d0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "doc_id                  30\n",
              "question                30\n",
              "answer_snippet          30\n",
              "source                  30\n",
              "confidence_indicator    30\n",
              "last_updated            30\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>doc_id</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>question</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer_snippet</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>source</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>confidence_indicator</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>last_updated</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = 'YOUR_API_KEY'\n",
        "# client = OpenAI()\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   # Check if text is not empty or just whitespace\n",
        "   if text.strip():\n",
        "       try:\n",
        "           return client.embeddings.create(input = [text], model=AZURE_DEPLOYMENT_NAME).data[0].embedding\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error getting embedding for text: {text}. Error: {e}\")\n",
        "           return None\n",
        "   else:\n",
        "       return None\n",
        "\n",
        "df['answer_embedding'] = df['answer_snippet'].apply(lambda x: get_embedding(x))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "7-qtWow1ouSf",
        "outputId": "13448e93-dbc5-42b1-e0d6-a5995141dd5b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  doc_id                                           question  \\\n",
              "0  KB001             What are best practices for debugging?   \n",
              "1  KB002    What are best practices for performance tuning?   \n",
              "2  KB003               What are best practices for caching?   \n",
              "3  KB004  What are best practices for asynchronous progr...   \n",
              "4  KB005        What are best practices for API versioning?   \n",
              "\n",
              "                                      answer_snippet  \\\n",
              "0  When addressing debugging, it's important to f...   \n",
              "1  When addressing performance tuning, it's impor...   \n",
              "2  When addressing caching, it's important to fol...   \n",
              "3  When addressing asynchronous programming, it's...   \n",
              "4  When addressing API versioning, it's important...   \n",
              "\n",
              "                              source confidence_indicator last_updated  \\\n",
              "0                 debugging_guide.md             moderate   2024-01-10   \n",
              "1        performance tuning_guide.md             moderate   2024-02-10   \n",
              "2                   caching_guide.md             moderate   2024-03-10   \n",
              "3  asynchronous programming_guide.md             moderate   2024-04-10   \n",
              "4            API versioning_guide.md             moderate   2024-05-10   \n",
              "\n",
              "                                    answer_embedding  \n",
              "0  [-0.011498448438942432, 0.024632882326841354, ...  \n",
              "1  [-0.001747551723383367, 0.0041742464527487755,...  \n",
              "2  [0.002506050281226635, 0.02132047526538372, 0....  \n",
              "3  [-0.017202088609337807, -0.006009227130562067,...  \n",
              "4  [0.004086621105670929, -0.008509399369359016, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51e83ae6-e50f-4464-ae5f-db20c4a25b9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_snippet</th>\n",
              "      <th>source</th>\n",
              "      <th>confidence_indicator</th>\n",
              "      <th>last_updated</th>\n",
              "      <th>answer_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KB001</td>\n",
              "      <td>What are best practices for debugging?</td>\n",
              "      <td>When addressing debugging, it's important to f...</td>\n",
              "      <td>debugging_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-01-10</td>\n",
              "      <td>[-0.011498448438942432, 0.024632882326841354, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KB002</td>\n",
              "      <td>What are best practices for performance tuning?</td>\n",
              "      <td>When addressing performance tuning, it's impor...</td>\n",
              "      <td>performance tuning_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-02-10</td>\n",
              "      <td>[-0.001747551723383367, 0.0041742464527487755,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KB003</td>\n",
              "      <td>What are best practices for caching?</td>\n",
              "      <td>When addressing caching, it's important to fol...</td>\n",
              "      <td>caching_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-03-10</td>\n",
              "      <td>[0.002506050281226635, 0.02132047526538372, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KB004</td>\n",
              "      <td>What are best practices for asynchronous progr...</td>\n",
              "      <td>When addressing asynchronous programming, it's...</td>\n",
              "      <td>asynchronous programming_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-04-10</td>\n",
              "      <td>[-0.017202088609337807, -0.006009227130562067,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KB005</td>\n",
              "      <td>What are best practices for API versioning?</td>\n",
              "      <td>When addressing API versioning, it's important...</td>\n",
              "      <td>API versioning_guide.md</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2024-05-10</td>\n",
              "      <td>[0.004086621105670929, -0.008509399369359016, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51e83ae6-e50f-4464-ae5f-db20c4a25b9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-51e83ae6-e50f-4464-ae5f-db20c4a25b9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-51e83ae6-e50f-4464-ae5f-db20c4a25b9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0a75da5b-e40d-436c-bd87-0cf2d7f55761\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a75da5b-e40d-436c-bd87-0cf2d7f55761')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0a75da5b-e40d-436c-bd87-0cf2d7f55761 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"KB028\",\n          \"KB016\",\n          \"KB024\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"What are best practices for error handling?\",\n          \"What are best practices for performance tuning?\",\n          \"What are best practices for unit testing?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_snippet\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"When addressing error handling, it's important to follow well-defined patterns...\",\n          \"When addressing performance tuning, it's important to follow well-defined patterns...\",\n          \"When addressing unit testing, it's important to follow well-defined patterns...\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"error handling_guide.md\",\n          \"performance tuning_guide.md\",\n          \"unit testing_guide.md\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"confidence_indicator\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"moderate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_updated\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"2024-01-10\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnUyEQOL6SEf",
        "outputId": "d38c439d-e185-4c22-ce8a-cfd76d6c4f96"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/327.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.7/327.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai langgraph dotenv --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPc3rFqVsCz2",
        "outputId": "2280621e-f3a7-445e-fd91-d942908f669c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "GRGvtDElr2Bz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://42589af0-252c-4406-af8f-138a66e7ebd6.us-west-1-0.aws.cloud.qdrant.io:6333\",\n",
        "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.x_9qUfpk8fyUI3SijtVffgcNJlDLk186e8_6PJZyluc\",\n",
        ")\n",
        "\n",
        "print(qdrant_client.get_collections())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw2YoI9M1UDW",
        "outputId": "8b4c02b3-afd0-4528-b0e4-f1e3538cfb4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collections=[CollectionDescription(name='kb_index')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
        "from qdrant_client import models\n",
        "\n",
        "# Create an index (collection)\n",
        "collection_name = \"kb_index\"\n",
        "\n",
        "# Get the dimension of the embeddings from the first valid embedding\n",
        "embedding_dimension = len(df.loc[df['answer_embedding'].notna(), 'answer_embedding'].iloc[0])\n",
        "\n",
        "qdrant_client.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=embedding_dimension, distance=Distance.COSINE)\n",
        ")\n",
        "\n",
        "# Prepare data for upserting\n",
        "points_to_upsert = []\n",
        "for index, row in df.iterrows():\n",
        "    if row['answer_embedding'] is not None:\n",
        "        points_to_upsert.append(\n",
        "            PointStruct(\n",
        "                id=index,  # Use the DataFrame index as the point ID\n",
        "                vector=row['answer_embedding'],\n",
        "                payload={\"answer_snippet\": row['answer_snippet']} # Store the answer snippet in the payload\n",
        "            )\n",
        "        )\n",
        "\n",
        "# Upsert the data in batches\n",
        "batch_size = 100\n",
        "for i in tqdm(range(0, len(points_to_upsert), batch_size), desc=\"Upserting to Qdrant\"):\n",
        "    batch = points_to_upsert[i:i + batch_size]\n",
        "    qdrant_client.upsert(\n",
        "        collection_name=collection_name,\n",
        "        wait=True,\n",
        "        points=batch\n",
        "    )\n",
        "\n",
        "print(f\"Upserted {len(points_to_upsert)} points to collection '{collection_name}'.\")\n",
        "print(qdrant_client.count(collection_name=collection_name, exact=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqhABphc7rux",
        "outputId": "cff99fc6-b6a4-430b-ca41-19d5c1de602c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Upserting to Qdrant: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 30 points to collection 'kb_index'.\n",
            "count=30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Retriever Node\n",
        "!pip install -U langgraph --quiet\n",
        "from langgraph.graph import Graph\n",
        "from typing import List, Dict\n",
        "\n",
        "# Modify retrieve_kb to accept the state dictionary and extract user_question\n",
        "def retrieve_kb(state: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Retrieves relevant knowledge base hits based on the user's question from the state.\n",
        "    \"\"\"\n",
        "    user_question = state.get(\"user_question\", \"\")\n",
        "    if not user_question:\n",
        "        return {\"kb_hits\": []}\n",
        "\n",
        "    question_embedding = get_embedding(user_question)\n",
        "    if question_embedding is None:\n",
        "        return {\"kb_hits\": []}\n",
        "\n",
        "    # Assuming qdrant_client and collection_name are defined above\n",
        "    search_result = qdrant_client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=question_embedding,\n",
        "        limit=5  # Get the top 5 most similar answer_snippet vectors\n",
        "    )\n",
        "\n",
        "    kb_hits = []\n",
        "    for hit in search_result:\n",
        "        # Assuming your payload contains 'answer_snippet' and 'source'\n",
        "        # Modify based on your actual payload structure\n",
        "        kb_hits.append({\n",
        "            \"doc_id\": hit.id,\n",
        "            \"answer_snippet\": hit.payload.get(\"answer_snippet\", \"\"),\n",
        "            # You might need to add a 'source' field to your Qdrant payload\n",
        "            # if it's not already there.\n",
        "            \"source\": hit.payload.get(\"source\", \"unknown\")\n",
        "        })\n",
        "    return {\"kb_hits\": kb_hits}\n",
        "\n",
        "# Define the graph\n",
        "workflow = Graph()\n",
        "\n",
        "# Add the retrieve_kb node\n",
        "workflow.add_node(\"retrieve_kb\", retrieve_kb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ggg-AhXraRB",
        "outputId": "1a97ca0e-273f-4e79-e6d7-4f66dd598e68"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.graph.Graph at 0x78e105698050>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. LLM Answer Node\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "import os # Import the os module to access environment variables\n",
        "\n",
        "# Define the prompt template for the LLM Answer Node\n",
        "llm_answer_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are a software best-practices assistant.\n",
        "User Question:\n",
        "{user_question}\n",
        "Retrieved Snippets:\n",
        "{kb_snippets}\n",
        "Task:\n",
        "Based on these snippets, write a concise answer to the user’s question.\n",
        "Cite each snippet you use by its doc_id in square brackets (e.g., [KB004]).\n",
        "Return only the answer text.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Helper function to format KB hits for the prompt\n",
        "def format_kb_hits_for_prompt(kb_hits: List[Dict]) -> str:\n",
        "    formatted_snippets = []\n",
        "    for hit in kb_hits:\n",
        "        formatted_snippets.append(f\"[{hit['doc_id']}] {hit['answer_snippet']}\")\n",
        "    return \"\\n\".join(formatted_snippets)\n",
        "\n",
        "# Define the LLM Answer Node function\n",
        "def generate_answer(user_question: str, kb_hits: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Generates an initial answer based on retrieved knowledge base snippets.\n",
        "    \"\"\"\n",
        "    llm = AzureChatOpenAI(\n",
        "        azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "        deployment_name=\"gpt-4\" # Or \"gpt-3.5-turbo\" based on your preference\n",
        "    )\n",
        "\n",
        "    chain = (\n",
        "        {\n",
        "            \"user_question\": lambda x: x[\"user_question\"],\n",
        "            \"kb_snippets\": lambda x: format_kb_hits_for_prompt(x[\"kb_hits\"])\n",
        "        }\n",
        "        | llm_answer_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    response = chain.invoke({\"user_question\": user_question, \"kb_hits\": kb_hits})\n",
        "    return {\"initial_answer\": response}\n",
        "\n",
        "\n",
        "# Add the generate_answer node\n",
        "workflow.add_node(\"generate_answer\", generate_answer)\n",
        "\n",
        "# Add an edge from retrieve_kb to generate_answer\n",
        "workflow.add_edge('retrieve_kb', 'generate_answer')\n",
        "\n",
        "# Set the entry point\n",
        "workflow.set_entry_point(\"retrieve_kb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31b63f5-96dc-4875-f208-22f722c2bb1a",
        "id": "fGu2E-r3roAC"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.graph.Graph at 0x78e105698050>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. critique_answer\n",
        "\n",
        "# Define the prompt template for the Self-Critique Node\n",
        "critique_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are a critical QA assistant. The user asked: {user_question}\n",
        "Initial Answer:\n",
        "{initial_answer}\n",
        "KB Snippets:\n",
        "{kb_snippets}\n",
        "Task:\n",
        "Determine if the initial answer fully addresses the question using only these snippets.\n",
        "- If it does, respond exactly: COMPLETE\n",
        "- If it misses any point or cites missing info, respond: REFINE: <short list of missing topic keywords>\n",
        "Return exactly one line.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Define the Self-Critique Node function\n",
        "def critique_answer(user_question: str, initial_answer: str, kb_hits: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Critiques the initial answer based on the retrieved knowledge base snippets.\n",
        "    \"\"\"\n",
        "    llm = AzureChatOpenAI(\n",
        "        azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "        deployment_name=\"gpt-4\"\n",
        "    )\n",
        "\n",
        "    chain = (\n",
        "        {\n",
        "            \"user_question\": lambda x: x[\"user_question\"],\n",
        "            \"initial_answer\": lambda x: x[\"initial_answer\"],\n",
        "            \"kb_snippets\": lambda x: format_kb_hits_for_prompt(x[\"kb_hits\"])\n",
        "        }\n",
        "        | critique_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    critique_result = chain.invoke({\"user_question\": user_question, \"initial_answer\": initial_answer, \"kb_hits\": kb_hits})\n",
        "    return {\"critique_result\": critique_result}\n",
        "\n",
        "# Add the critique_answer node\n",
        "workflow.add_node(\"critique_answer\", critique_answer)\n",
        "\n",
        "# Add an edge from generate_answer to critique_answer\n",
        "workflow.add_edge('generate_answer', 'critique_answer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfy5jBswrkr4",
        "outputId": "d29ac742-a0e0-4acc-e92c-799932fd3f68"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.graph.Graph at 0x78e105698050>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QzOBO7qarkpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V9zzL1WUrkm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdBIQtVIrkkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dpq2Z_abrkeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BLjSi3tOrkXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: define a LangGraph graph with these four\n",
        "# nodes: first node is -\n",
        "# 1. Retriever Node\n",
        "# Name: retrieve_kb\n",
        "# Type: “vector_retriever” (wraps a simple Python function).\n",
        "# Input:\n",
        "# user_question: str\n",
        "# Operation:\n",
        "# 1. Embed user_question.\n",
        "# 2. Query the vector index for the top 5 most similar answer_snippet vectors.\n",
        "# 3. Return a list of up to 5 hits, each { \"doc_id\": <id>, \"answer_snippet\": <text>, \"source\": <filename> }.\n",
        "# Output:\n",
        "# kb_hits: List[Dict]\n",
        "# 1. Retriever Node\n",
        "!pip install -U langgraph --quiet\n",
        "from langgraph.graph import Graph\n",
        "from typing import List, Dict\n",
        "\n",
        "# Modify retrieve_kb to accept the state dictionary and extract user_question\n",
        "def retrieve_kb(state: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Retrieves relevant knowledge base hits based on the user's question from the state.\n",
        "    \"\"\"\n",
        "    user_question = state.get(\"user_question\", \"\")\n",
        "    if not user_question:\n",
        "        return {\"kb_hits\": []}\n",
        "\n",
        "    question_embedding = get_embedding(user_question)\n",
        "    if question_embedding is None:\n",
        "        return {\"kb_hits\": []}\n",
        "\n",
        "    # Assuming qdrant_client and collection_name are defined above\n",
        "    search_result = qdrant_client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=question_embedding,\n",
        "        limit=5  # Get the top 5 most similar answer_snippet vectors\n",
        "    )\n",
        "\n",
        "    kb_hits = []\n",
        "    for hit in search_result:\n",
        "        # Assuming your payload contains 'answer_snippet' and 'source'\n",
        "        # Modify based on your actual payload structure\n",
        "        kb_hits.append({\n",
        "            \"doc_id\": hit.id,\n",
        "            \"answer_snippet\": hit.payload.get(\"answer_snippet\", \"\"),\n",
        "            # You might need to add a 'source' field to your Qdrant payload\n",
        "            # if it's not already there.\n",
        "            \"source\": hit.payload.get(\"source\", \"unknown\")\n",
        "        })\n",
        "    return {\"kb_hits\": kb_hits}\n",
        "\n",
        "# Define the graph\n",
        "workflow = Graph()\n",
        "\n",
        "# Add the retrieve_kb node\n",
        "workflow.add_node(\"retrieve_kb\", retrieve_kb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r51PavB98Et7",
        "outputId": "bb4ecf96-ba38-4ae2-be5e-f05f881902c1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.graph.Graph at 0x78e11195ea50>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: second langgraph node is -LLM Answer Node\n",
        "# Name: generate_answer\n",
        "# Type: LLMChainNode\n",
        "# Inputs:\n",
        "# user_question: str\n",
        "# kb_hits: List[Dict]\n",
        "# Prompt Template (example):\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "import os # Import the os module to access environment variables\n",
        "\n",
        "# Define the prompt template for the LLM Answer Node\n",
        "llm_answer_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are a software best-practices assistant.\n",
        "User Question:\n",
        "{user_question}\n",
        "Retrieved Snippets:\n",
        "{kb_snippets}\n",
        "Task:\n",
        "Based on these snippets, write a concise answer to the user’s question.\n",
        "Cite each snippet you use by its doc_id in square brackets (e.g., [KB004]).\n",
        "Return only the answer text.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Helper function to format KB hits for the prompt\n",
        "def format_kb_hits_for_prompt(kb_hits: List[Dict]) -> str:\n",
        "    formatted_snippets = []\n",
        "    for hit in kb_hits:\n",
        "        formatted_snippets.append(f\"[{hit['doc_id']}] {hit['answer_snippet']}\")\n",
        "    return \"\\n\".join(formatted_snippets)\n",
        "\n",
        "# Define the LLM Answer Node function\n",
        "def generate_answer(user_question: str, kb_hits: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Generates an initial answer based on retrieved knowledge base snippets.\n",
        "    \"\"\"\n",
        "    llm = AzureChatOpenAI(\n",
        "        azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "        deployment_name=\"gpt-4\" # Or \"gpt-3.5-turbo\" based on your preference\n",
        "    )\n",
        "\n",
        "    chain = (\n",
        "        {\n",
        "            \"user_question\": lambda x: x[\"user_question\"],\n",
        "            \"kb_snippets\": lambda x: format_kb_hits_for_prompt(x[\"kb_hits\"])\n",
        "        }\n",
        "        | llm_answer_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    response = chain.invoke({\"user_question\": user_question, \"kb_hits\": kb_hits})\n",
        "    return {\"initial_answer\": response}\n",
        "\n",
        "\n",
        "# Add the generate_answer node\n",
        "workflow.add_node(\"generate_answer\", generate_answer)\n",
        "\n",
        "# Add an edge from retrieve_kb to generate_answer\n",
        "workflow.add_edge('retrieve_kb', 'generate_answer')\n",
        "\n",
        "# Set the entry point\n",
        "workflow.set_entry_point(\"retrieve_kb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZasepHM2CQWu",
        "outputId": "13097526-598b-46ad-dc93-6926588b3c89"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.graph.Graph at 0x7a9e6b161a50>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "956f7835"
      },
      "source": [
        "!pip install langchain-openai --quiet"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: third node is Self-Critique Node\n",
        "# Name: critique_answer\n",
        "# Type: LLMChainNode\n",
        "# Inputs:\n",
        "# user_question: str\n",
        "# initial_answer: str\n",
        "# kb_hits: List[Dict]\n",
        "# Prompt Template (example):\n",
        "# You are a critical QA assistant. The user asked: {user_question}\n",
        "# Initial Answer:\n",
        "# {initial_answer}\n",
        "# KB Snippets:\n",
        "# {for hit in kb_hits: print(f\"[{hit['doc_id']}] {hit['answer_snippet']}\")}\n",
        "# Task:\n",
        "# Determine if the initial answer fully addresses the question using only these snippets.\n",
        "# - If it does, respond exactly: COMPLETE\n",
        "# - If it misses any point or cites missing info, respond: REFINE: <short list of missing topic keywords>\n",
        "# Return exactly one line.\n",
        "# LLM Settings:\n",
        "# Model: gpt-4\n",
        "# Temperature: 0\n",
        "# Output:\n",
        "# critique_result: str (either \"COMPLETE\" or \"REFINE: ...\")\n",
        "\n",
        "# 3. critique_answer\n",
        "\n",
        "# Define the prompt template for the Self-Critique Node\n",
        "critique_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are a critical QA assistant. The user asked: {user_question}\n",
        "Initial Answer:\n",
        "{initial_answer}\n",
        "KB Snippets:\n",
        "{kb_snippets}\n",
        "Task:\n",
        "Determine if the initial answer fully addresses the question using only these snippets.\n",
        "- If it does, respond exactly: COMPLETE\n",
        "- If it misses any point or cites missing info, respond: REFINE: <short list of missing topic keywords>\n",
        "Return exactly one line.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Define the Self-Critique Node function\n",
        "def critique_answer(user_question: str, initial_answer: str, kb_hits: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Critiques the initial answer based on the retrieved knowledge base snippets.\n",
        "    \"\"\"\n",
        "    llm = AzureChatOpenAI(\n",
        "        azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "        deployment_name=\"gpt-4\"\n",
        "    )\n",
        "\n",
        "    chain = (\n",
        "        {\n",
        "            \"user_question\": lambda x: x[\"user_question\"],\n",
        "            \"initial_answer\": lambda x: x[\"initial_answer\"],\n",
        "            \"kb_snippets\": lambda x: format_kb_hits_for_prompt(x[\"kb_hits\"])\n",
        "        }\n",
        "        | critique_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    critique_result = chain.invoke({\"user_question\": user_question, \"initial_answer\": initial_answer, \"kb_hits\": kb_hits})\n",
        "    return {\"critique_result\": critique_result}\n",
        "\n",
        "# Add the critique_answer node\n",
        "workflow.add_node(\"critique_answer\", critique_answer)\n",
        "\n",
        "# Add an edge from generate_answer to critique_answer\n",
        "workflow.add_edge('generate_answer', 'critique_answer')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHyp9gE6DKTH",
        "outputId": "6a155e33-7aff-4178-eb7b-9a22c9b149ce"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.graph.Graph at 0x7a9e6b161a50>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Forth node is\n",
        "# Refinement Node\n",
        "# Name: refine_answer\n",
        "# Type: LLMChainNode\n",
        "# Inputs:\n",
        "# user_question: str\n",
        "# initial_answer: str\n",
        "# critique_result: str\n",
        "# kb_hits: List[Dict]\n",
        "# Operation:\n",
        "# 1. Extract missing‐topic keywords from critique_result (e.g., “cache invalidation”).\n",
        "# 2. Build a new query string:\n",
        "# new_query = f\"{user_question} and information on {missing_keywords}\"\n",
        "# 3. Call the same retriever function to get one additional snippet (top_k=1) for that new_query.\n",
        "# Prompt Template (example):\n",
        "# You are a software best-practices assistant refining your answer. The user asked: {user_question}\n",
        "# Initial Answer:\n",
        "# {initial_answer}\n",
        "# Critique: {critique_result}\n",
        "# Additional Snippet:\n",
        "# [Code to display the single additional snippet’s doc_id and text]\n",
        "# Task:\n",
        "# Incorporate this snippet into the answer, covering the missing points.\n",
        "# Cite any snippet you use by doc_id in square brackets.\n",
        "# Return only the final refined answer.\n",
        "# LLM Settings:\n",
        "# Model: gpt-4\n",
        "# Temperature: 0\n",
        "# Output:\n",
        "# refined_answer: str\n",
        "\n",
        "#4. Refinement Node\n",
        "\n",
        "# Define the prompt template for the Refine Answer Node\n",
        "refine_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"You are a software best-practices assistant refining your answer. The user asked: {user_question}\n",
        "Initial Answer:\n",
        "{initial_answer}\n",
        "Critique: {critique_result}\n",
        "Additional Snippet:\n",
        "[{additional_snippet_doc_id}] {additional_snippet_text}\n",
        "Task:\n",
        "Incorporate this snippet into the answer, covering the missing points.\n",
        "Cite any snippet you use by doc_id in square brackets.\n",
        "Return only the final refined answer.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Define the Refine Answer Node function\n",
        "def refine_answer(user_question: str, initial_answer: str, critique_result: str, kb_hits: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Refines the initial answer based on the critique and an additional retrieved snippet.\n",
        "    \"\"\"\n",
        "    # 1. Extract missing-topic keywords from critique_result\n",
        "    missing_keywords = \"\"\n",
        "    if critique_result.startswith(\"REFINE: \"):\n",
        "        missing_keywords = critique_result[len(\"REFINE: \"):].strip()\n",
        "\n",
        "    additional_snippet = None\n",
        "    if missing_keywords:\n",
        "        # 2. Build a new query string\n",
        "        new_query = f\"{user_question} and information on {missing_keywords}\"\n",
        "\n",
        "        # 3. Call the same retriever function to get one additional snippet\n",
        "        additional_hits = retrieve_kb(new_query)\n",
        "        if additional_hits:\n",
        "            additional_snippet = additional_hits[0] # Get the top 1\n",
        "\n",
        "    llm = AzureChatOpenAI(\n",
        "        azure_endpoint=os.getenv(\"OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "        api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "        deployment_name=\"gpt-4\"\n",
        "    )\n",
        "\n",
        "    # Prepare input for the refine prompt\n",
        "    refine_input = {\n",
        "        \"user_question\": user_question,\n",
        "        \"initial_answer\": initial_answer,\n",
        "        \"critique_result\": critique_result,\n",
        "        \"additional_snippet_doc_id\": additional_snippet['doc_id'] if additional_snippet else \"N/A\",\n",
        "        \"additional_snippet_text\": additional_snippet['answer_snippet'] if additional_snippet else \"No additional relevant snippet found.\"\n",
        "    }\n",
        "\n",
        "\n",
        "    chain = (\n",
        "        refine_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    refined_answer = chain.invoke(refine_input)\n",
        "    return {\"refined_answer\": refined_answer}\n",
        "\n",
        "# Add the refine_answer node\n",
        "workflow.add_node(\"refine_answer\", refine_answer)\n",
        "\n",
        "# Define conditional edge based on critique_result\n",
        "def route_critique(state: Dict) -> str:\n",
        "    \"\"\"\n",
        "    Routes based on the critique result.\n",
        "    \"\"\"\n",
        "    if state.get(\"critique_result\", \"\").startswith(\"COMPLETE\"):\n",
        "        return \"complete\"\n",
        "    else:\n",
        "        return \"refine\"\n",
        "\n",
        "# Add a conditional edge from critique_answer\n",
        "workflow.add_conditional_edge(\n",
        "    'critique_answer',\n",
        "    route_critique,\n",
        "    {\n",
        "        \"complete\": '__end__', # End the graph if the critique is complete\n",
        "        \"refine\": 'refine_answer' # Route to refine_answer if critique suggests refinement\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add an edge from refine_answer to the end\n",
        "workflow.add_edge('refine_answer', '__end__')\n",
        "\n"
      ],
      "metadata": {
        "id": "bXhJ9mgKDqX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code for Graph Control Flow\n",
        "# Wire nodes in sequence:\n",
        "# 1. retrieve_kb → generate_answer → critique_answer.\n",
        "# 2. Add a simple Decision node (or an if check in your driver script) that:\n",
        "# If critique_result == \"COMPLETE\", take initial_answer as final.\n",
        "# If critique_result.startswith(\"REFINE\"), call the refinement logic (retrieve+refine) to produce refined_answer.\n",
        "# 3. Wrap whichever answer (initial or refined) into a JSON response:\n",
        "# { \"answer\": \"<final_answer_text>\" }\n",
        "\n",
        "# Set the entry point for the workflow\n",
        "workflow.set_entry_point(\"retrieve_kb\")\n",
        "\n",
        "# Build the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "# Example usage:\n",
        "user_question = \"What are the principles of Clean Architecture?\"\n",
        "# The input to the graph is the user question, passed to the entry point 'retrieve_kb'\n",
        "inputs = {\"user_question\": user_question}\n",
        "\n",
        "# Execute the graph\n",
        "# The result will be available in the state of the last node executed.\n",
        "# Since the graph ends with either '__end__', we need to inspect the state\n",
        "# to get the final answer, which will be either 'initial_answer' or 'refined_answer'.\n",
        "final_state = app.invoke(inputs)\n",
        "\n",
        "# Determine the final answer from the final state\n",
        "final_answer = final_state.get(\"refined_answer\", final_state.get(\"initial_answer\", \"No answer generated.\"))\n",
        "\n",
        "# Wrap the final answer in a JSON-like structure (Python dictionary)\n",
        "response_json = {\"answer\": final_answer}\n",
        "\n",
        "import json\n",
        "print(json.dumps(response_json, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "M8L3jCSKENc7",
        "outputId": "cc318dc5-3ec6-441e-e13a-d4b1e6891add"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langgraph.graph.graph:Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Already found path for node '__start__'.\nFor multiple edges, use StateGraph with an Annotated state key.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-502172050>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Set the entry point for the workflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_entry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retrieve_kb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Build the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/graph/graph.py\u001b[0m in \u001b[0;36mset_entry_point\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowing\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mchaining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     def set_conditional_entry_point(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/graph/graph.py\u001b[0m in \u001b[0;36madd_edge\u001b[0;34m(self, start_key, end_key)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         ):\n\u001b[0;32m--> 144\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0;34mf\"Already found path for node '{start_key}'.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;34m\"For multiple edges, use StateGraph with an Annotated state key.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Already found path for node '__start__'.\nFor multiple edges, use StateGraph with an Annotated state key."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the graph\n",
        "builder = StateGraph()\n",
        "builder.add_node(\"retrieve_kb\",chat)\n",
        "builder.add_edge(START,\"chat1\")\n",
        "builder.add_edge(\"chat1\",END) #Optional here to mention END\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "\n",
        "\n",
        "retrieve_kb\n",
        "2. generate_answer\n",
        "3. critique_answer\n",
        "4. refine_answer"
      ],
      "metadata": {
        "id": "bp0wmXmxG1C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_kb"
      ],
      "metadata": {
        "id": "df2mH6bCHFk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the graph\n",
        "builder = StateGraph()\n",
        "builder.add_node(\"retrieve_kb\",chat)\n",
        "builder.add_edge(START,\"retrieve_kb\")\n",
        "builder.add_edge(\"retrieve_kb\",END) #Optional here to mention END\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "hFhdCtaKHQwb",
        "outputId": "7fbf2ad5-9b49-439e-d5b8-94e5f95cf00c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Must provide state_schema or input and output",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-2295868473>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retrieve_kb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"retrieve_kb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"retrieve_kb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Optional here to mention END\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_schema, config_schema, input, output)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_schema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must provide state_schema or input and output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mstate_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValueError\u001b[0m: Must provide state_schema or input and output"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import AzureChatOpenAI\n"
      ],
      "metadata": {
        "id": "cj9raa2LHchV"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}